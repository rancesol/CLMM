{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit halo mass to shear profile using Numcosmo statistical framework\n",
    "\n",
    "_the LSST-DESC CLMM team_\n",
    "\n",
    "This notebook is used to make Fig.4 of the CLMM v1.0 paper and is a shortened version of the `Example2_Fit_Halo_Mass_to_Shear_Catalog.ipynb` notebook. Please refer to the `Example2` notebook for more details.\n",
    "\n",
    "NB: to display the corner plot output of the MCMC analysis, you will need the `corner` package installed in your python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import some standard packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For NumCosmo\n",
    "import os\n",
    "import sys\n",
    "import gi\n",
    "\n",
    "gi.require_version('NumCosmo', '1.0')\n",
    "gi.require_version('NumCosmoMath', '1.0')\n",
    "from gi.repository import GObject\n",
    "from gi.repository import NumCosmo as Nc\n",
    "from gi.repository import NumCosmoMath as Ncm\n",
    "\n",
    "from scipy.stats import chi2\n",
    "\n",
    "import math\n",
    "# The corner package is needed to view the results of the MCMC analysis\n",
    "import corner\n",
    "\n",
    "\n",
    "os.environ['CLMM_MODELING_BACKEND'] = 'nc'\n",
    "\n",
    "__name__ = \"NcContext\"\n",
    "\n",
    "Ncm.cfg_init ()\n",
    "Ncm.cfg_set_log_handler (lambda msg: sys.stdout.write (msg) and sys.stdout.flush ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: import clmm\n",
    "except:\n",
    "    import notebook_install\n",
    "    notebook_install.install_clmm_pipeline(upgrade=False)\n",
    "    import clmm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from clmm.support.sampler import fitters\n",
    "\n",
    "clmm.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import `clmm`'s core modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clmm.dataops as da\n",
    "import clmm.galaxycluster as gc\n",
    "import clmm.theory as theory\n",
    "from clmm import Cosmology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then import a support modules for a specific data sets.\n",
    "`clmm` includes support modules that enable the user to generate mock data in a format compatible with `clmm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clmm.support import mock_data as mock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making mock data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create mock data, we need to define a true cosmology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_cosmo = Cosmology(H0 = 70.0, Omega_dm0 = 0.27 - 0.045, Omega_b0 = 0.045, Omega_k0 = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set some parameters for a mock galaxy cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmo = mock_cosmo\n",
    "cluster_m = 1.e15 # M200,m [Msun]\n",
    "cluster_z = 0.3   # Cluster's redshift\n",
    "concentration = 4\n",
    "ngals = 10000     # Number of galaxies\n",
    "Delta = 200\n",
    "cluster_ra = 0.0\n",
    "cluster_dec = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the `mock_data` support module to generate a background galaxy catalog, with galaxies distributed in redshift according to the Chang et al. (2013) parametrisation, and that includes (a low level of) shape noise and photoz errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_data_z = mock.generate_galaxy_catalog(cluster_m, cluster_z, concentration, cosmo, 'chang13', \n",
    "                                            shapenoise=0.05, \n",
    "                                            photoz_sigma_unscaled=0.05, ngals=ngals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The galaxy catalog table is used to instatiate a `clmm.GalaxyCluster` object and is stored in the `galcat` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = \"CL\"\n",
    "gc_object = clmm.GalaxyCluster(cluster_id, cluster_ra, cluster_dec,\n",
    "                               cluster_z, noisy_data_z)\n",
    "gc_object.save('noisy_GC_z.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = clmm.GalaxyCluster.load('noisy_GC_z.pkl')\n",
    "\n",
    "print(\"Cluster info = ID:\", cl.unique_id, \"; ra:\", cl.ra, \"; dec:\", cl.dec, \"; z_l :\", cl.z)\n",
    "print(\"The number of source galaxies is :\", len(cl.galcat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving observables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing shear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`clmm.dataops.compute_tangential_and_cross_components` calculates the tangential and cross shears for each source galaxy in the cluster object and adds the corresponding columns to the `galcat` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.compute_tangential_and_cross_components(geometry=\"flat\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radially binning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = da.make_bins(0.7, 4, 15, method='evenlog10width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`clmm.dataops.make_radial_profile` evaluates the average shear of the galaxy catalog in bins of radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.make_radial_profile(\"Mpc\", bins=bin_edges,cosmo=cosmo);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running `clmm.dataops.make_radial_profile` on a `clmm.GalaxyCluster` object, the object acquires the `clmm.GalaxyCluster.profile` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in cl.profile.colnames: cl.profile[n].format = \"%6.3e\"\n",
    "cl.profile.pprint(max_width=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsize = 14\n",
    "plt.errorbar(cl.profile['radius'], cl.profile['gt'], yerr=cl.profile['gt_err'])\n",
    "\n",
    "plt.title(r'Binned reduced tangential shear profile', fontsize=fsize)\n",
    "plt.xlabel(r'$r\\;[Mpc]$', fontsize=fsize)\n",
    "plt.ylabel(r'$g_t$', fontsize=fsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a halo mass - highlighting bias when NOT accounting for the source redshift distribution in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate the best-fit mass using a simple implementation of the likelihood using a NcmDataGaussDiag object.\n",
    "\n",
    "Here, to build the model we purposely make the WRONG assumption that the average shear in bin $i$ equals the shear at the average redshift in the bin; i.e. we assume that $\\langle g_t\\rangle_i = g_t(\\langle z\\rangle_i)$. This will yield a bias in the reconstructed mass where the sources followed the Chang et al. (2013) distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CLMM theory object-oriented interface is used to build the model and we also use NumCosmo statistical framework to perform the analysis. Below we create an object based on NumCosmo NcmDataGaussDiag (Gaussian likelihood with a diagonal covariance matrix) object. To connect with the C interface of NumCosmo the object must implement the methods: `do_get_length`, `do_get_dof`, `do_begin`, `do_prepare` and `do_mean_func`. The last method is responsible to compute the theoretical predictions. In the `param_set_ftype` calls below, one can change between FREE/FIXED to include/exclude the parameter from the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussGammaTErr (Ncm.DataGaussDiag):\n",
    "    z_cluster = GObject.Property (type = float, flags = GObject.PARAM_READWRITE)\n",
    "    z_source  = GObject.Property (type = Ncm.Vector, flags = GObject.PARAM_READWRITE)\n",
    "    r_source  = GObject.Property (type = Ncm.Vector, flags = GObject.PARAM_READWRITE)\n",
    "    z_err     = GObject.Property (type = Ncm.Vector, flags = GObject.PARAM_READWRITE)\n",
    "\n",
    "    def __init__ (self):\n",
    "        Ncm.DataGaussDiag.__init__ (self, n_points = 0)        \n",
    "        self.moo = clmm.Modeling ()\n",
    "    \n",
    "    def init_from_data (self, z_cluster, r_source, z_source, gt_profile, gt_err, z_err = None, moo = None):\n",
    "        \n",
    "        if moo:\n",
    "            self.moo = moo\n",
    "        \n",
    "        assert len (gt_profile) == len (z_source)\n",
    "        assert len (gt_profile) == len (r_source)\n",
    "        assert len (gt_profile) == len (gt_err)\n",
    "        \n",
    "        self.set_size (len (gt_profile))\n",
    "        \n",
    "        self.props.z_cluster = z_cluster\n",
    "        self.props.z_source  = Ncm.Vector.new_array (z_source)\n",
    "        self.props.r_source  = Ncm.Vector.new_array (r_source)\n",
    "        if z_err:\n",
    "            self.props.r_source  = Ncm.Vector.new_array (z_err)\n",
    "                \n",
    "        self.y.set_array (gt_profile)\n",
    "        \n",
    "        self.sigma.set_array (gt_err) # Diagonal covariance matrix: standard deviation values in gt_err.\n",
    "\n",
    "        \n",
    "        self.set_init (True)        \n",
    "    \n",
    "    # Once the NcmDataGaussDiag is initialized, its parent class variable np is set with the n_points value.\n",
    "    def do_get_length (self):\n",
    "        return self.np\n",
    "\n",
    "    def do_get_dof (self):\n",
    "        return self.np\n",
    "\n",
    "    def do_begin (self):\n",
    "        pass\n",
    "\n",
    "    def do_prepare (self, mset):\n",
    "        self.moo.set_mset (mset)\n",
    "        \n",
    "    def do_mean_func (self, mset, vp):\n",
    "        vp.set_array (self.moo.eval_reduced_tangential_shear (self.props.r_source.dup_array (), self.props.z_cluster, self.props.z_source.dup_array ()))\n",
    "        return\n",
    "\n",
    "GObject.type_register (GaussGammaTErr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model set (NcmMset), data set (NcmDataset) and NcmLikelihood objects to carry out a statistical analysis. \n",
    "\n",
    "The method `param_set_ftype` defines the parameters that can be fitted: `mid` - to which model set the parameter belongs to, `pid` - parameters' id, NcmParamType (FREE or FIXED) to say if the parameter will be fitted or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moo = clmm.Modeling (massdef = 'mean', delta_mdef = 200, halo_profile_model = 'nfw')\n",
    "moo.set_cosmo (cosmo)\n",
    "\n",
    "ggt = GaussGammaTErr ()\n",
    "\n",
    "ggt.init_from_data (z_cluster = cluster_z, r_source = cl.profile['radius'], z_source = cl.profile['z'], \n",
    "                    gt_profile = cl.profile['gt'], gt_err = cl.profile['gt_err'], moo = moo)\n",
    "\n",
    "mset = ggt.moo.get_mset ()\n",
    "\n",
    "#Parameters: cluster mass (log base 10) and concentration\n",
    "MDelta_pi = mset.param_get_by_full_name (\"NcHaloDensityProfile:log10MDelta\")\n",
    "cDelta_pi = mset.param_get_by_full_name (\"NcHaloDensityProfile:cDelta\")\n",
    "\n",
    "mset.param_set_ftype (MDelta_pi.mid, MDelta_pi.pid, Ncm.ParamType.FREE)\n",
    "mset.param_set_ftype (cDelta_pi.mid, cDelta_pi.pid, Ncm.ParamType.FREE)\n",
    "mset.prepare_fparam_map ()\n",
    "\n",
    "dset = Ncm.Dataset.new ()\n",
    "dset.append_data (ggt)\n",
    "lh = Ncm.Likelihood.new (dset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting parameters: Fisher Matrix\n",
    "\n",
    "The NcmFit object receives the NcmLikelihood and NcmMset objects. The user also indicates the fitting algorithm and the numerical differentiation method.  \n",
    "Functions `run` and `fisher` computes the [best-fit](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) and the [fisher matrix](https://en.wikipedia.org/wiki/Fisher_information#Multivariate_normal_distribution), respectively. `log_info` prints the complete information about the data used, models and its parameters, and `log_covar` prints the best-fit along with the error-bar and the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = Ncm.Fit.new (Ncm.FitType.NLOPT, \"ln-neldermead\", lh, mset, Ncm.FitGradType.NUMDIFF_FORWARD)\n",
    "fit.run (Ncm.FitRunMsgs.SIMPLE)\n",
    "fit.fisher ()\n",
    "fit.log_info ()\n",
    "fit.log_covar ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this 'wrong' model, the best-fit mass is biased low:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_est = 10**mset.param_get (MDelta_pi.mid, MDelta_pi.pid)\n",
    "m_est_err = fit.covar_sd (MDelta_pi.mid, MDelta_pi.pid) * m_est * math.log (10.0)\n",
    "\n",
    "print (r\"M = % 22.15e +/- %.0e M$_\\odot$\" % (m_est, m_est_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To investigate the result further we use the MCMC analysis below.\n",
    "\n",
    "We begin by specifying if the run is single- or multi-thread: `func_eval_set_max_threads` sets the maximum number of threads, and `func_eval_log_pool_stats` prints the information about the thread pool.\n",
    "\n",
    "Then, we initialize the transition kernel object (NcmMSetTransKern) which defines the distribution of the initial points of the parameter space to be used by the ensemble sampler. In this example we use the Gaussian transition kernel (NcmMSetTransKernGauss), with priors provided by the NcmMset (`set_prior_from_mset`). `set_cov_from_rescale` sets the covariance matrix with zero correlation and the diagonal terms defined by the scale of each parameter times the argument of `set_cov_from_rescale`. \n",
    "\n",
    "Here we use the Ensemble Sampler MCMC (ESMCMC) method. `nwalkers` and `walker` define the number of walkers and the algorithm used to move the points in the ensemble. \n",
    "Running: `start_run`, `run_lre` and `end_run`. `run_lre` runs the ESMCMC until the relative error of the mean of each parameter is smaller than $10^{-3}$. Its first argument (integer) indicates how many ensembles are computed before applying any convergence test.\n",
    "\n",
    "In the end we save the catalog to mcat_wrong to compare with a correct analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ncm.func_eval_set_max_threads (0)\n",
    "Ncm.func_eval_log_pool_stats ()\n",
    "\n",
    "init_sampler = Ncm.MSetTransKernGauss.new (0)\n",
    "init_sampler.set_mset (mset)\n",
    "init_sampler.set_prior_from_mset ()\n",
    "init_sampler.set_cov_from_rescale (1.0e-1)\n",
    "\n",
    "nwalkers = 100 # Number of walkers\n",
    "walker = Ncm.FitESMCMCWalkerAPS.new (nwalkers, mset.fparams_len ())\n",
    "\n",
    "# Ensemble Sampler MCMC\n",
    "esmcmc  = Ncm.FitESMCMC.new (fit, nwalkers, init_sampler, walker, Ncm.FitRunMsgs.SIMPLE)\n",
    "esmcmc.set_data_file (\"fig4_fit_wrong_esmcmc_out_aps.fits\")\n",
    "esmcmc.set_auto_trim (True)            # Detect and discard the burn-in points.\n",
    "esmcmc.set_auto_trim_div (100)\n",
    "esmcmc.set_max_runs_time (2.0 * 60.0)  # Maximum time between tests.\n",
    "\n",
    "esmcmc.start_run ()\n",
    "esmcmc.run_lre (20, 1.0e-2)\n",
    "esmcmc.end_run ()\n",
    "\n",
    "mcat_wrong = esmcmc.peek_catalog ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the cluster mass and concentration : using an unbinned analysis\n",
    "\n",
    "Here, instead of building an object directly on top of NcmDataGauss*, we use NumCosmo's framework to build non-binned likelihood for weak-lensing cluster analysis.\n",
    "\n",
    "For that we need two objects: a NcGalaxyWLReducedShearGauss that model a Gaussian distributed reduced shear likelihood, here the observables matrix is simply $(r, \\gamma_t, \\sigma_{\\gamma_t})$ for each galaxy. If the data has spectroscopic redshifts then we use NcGalaxyRedshiftSpec with an array of real redshifts. When photometric errors are included we use the NcGalaxyRedshiftGauss object that receives $(z, \\sigma_z)$ for each galaxy. \n",
    "\n",
    "Once we have the data objects ready we can proceed as in the previous examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nc_data_cluster_wl (theta, g_t, z_source, z_cluster, cosmo, dist, sigma_z = None, sigma_g = None):\n",
    "    r  = clmm.convert_units (theta, \"radians\", \"Mpc\", redshift = z_cluster, cosmo = cosmo)\n",
    "    ga = Ncm.ObjArray.new ()\n",
    "    \n",
    "    sigma_g = 1.0e-4 if not sigma_g else sigma_g\n",
    "    m_obs = np.column_stack ((r, g_t, np.repeat (sigma_g, len (r))))\n",
    "    \n",
    "    grsg = Nc.GalaxyWLReducedShearGauss (pos = Nc.GalaxyWLReducedShearGaussPos.R)\n",
    "    grsg.set_obs (Ncm.Matrix.new_array (m_obs.flatten (), 3))\n",
    "    \n",
    "    if sigma_z:\n",
    "        gzgs = Nc.GalaxyRedshiftGauss ()\n",
    "        z_obs = np.column_stack ((z_source, (1.0 + z_source) * sigma_z))\n",
    "        gzgs.set_obs (Ncm.Matrix.new_array (z_obs.flatten (), 2))\n",
    "    else:\n",
    "        gzgs = Nc.GalaxyRedshiftSpec ()\n",
    "        gzgs.set_z (Ncm.Vector.new_array (z_source))\n",
    "\n",
    "    gwl  = Nc.GalaxyWL (wl_dist = grsg, gz_dist = gzgs)\n",
    "    ga.add (gwl)\n",
    "\n",
    "    nc_dcwl = Nc.DataClusterWL (galaxy_array = ga, z_cluster = z_cluster)\n",
    "    nc_dcwl.set_init (True)\n",
    "    \n",
    "    return nc_dcwl\n",
    "\n",
    "def create_fit_obj (data_array, mset):\n",
    "    dset = Ncm.Dataset.new ()\n",
    "    for data in data_array:\n",
    "        dset.append_data (data)\n",
    "    lh = Ncm.Likelihood.new (dset)\n",
    "    fit = Ncm.Fit.new (Ncm.FitType.NLOPT, \"ln-neldermead\", lh, mset, Ncm.FitGradType.NUMDIFF_FORWARD)\n",
    "    #fit.set_params_reltol (1.0e-8)\n",
    "    #fit.set_m2lnL_reltol (1.0e-11)\n",
    "    \n",
    "    return fit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggt = create_nc_data_cluster_wl (cl.galcat['theta'], cl.galcat['et'], cl.galcat['z'], \n",
    "                                 cluster_z, cosmo, cosmo.dist, sigma_z = 0.05, sigma_g = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As the likelihood is not Gaussian, here we compute the [Observed Fisher Matrix](https://en.wikipedia.org/wiki/Observed_information) (`obs_fisher`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = create_fit_obj ([ggt], mset)\n",
    "fit.run (Ncm.FitRunMsgs.SIMPLE)\n",
    "fit.obs_fisher ()\n",
    "fit.log_covar ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the unbinned likelihood, the bestfit mass is not biased low anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_est = 10**mset.param_get (MDelta_pi.mid, MDelta_pi.pid)\n",
    "m_est_err = fit.covar_sd (MDelta_pi.mid, MDelta_pi.pid) * m_est * math.log (10.0)\n",
    "\n",
    "print (r\"M = % 22.15e +/- %.0e M$_\\odot$\" % (m_est, m_est_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As with the wrong model before, we investigate the results further with the MCMC analysis below. Depending on your machine, the next cell will take 15-30 min to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ncm.func_eval_set_max_threads (0)\n",
    "Ncm.func_eval_log_pool_stats ()\n",
    "\n",
    "init_sampler = Ncm.MSetTransKernGauss.new (0)\n",
    "init_sampler.set_mset (mset)\n",
    "init_sampler.set_prior_from_mset ()\n",
    "init_sampler.set_cov_from_rescale (1.0e-1)\n",
    "\n",
    "nwalkers = 100\n",
    "stretch = Ncm.FitESMCMCWalkerAPS.new (nwalkers, mset.fparams_len ())\n",
    "\n",
    "esmcmc  = Ncm.FitESMCMC.new (fit, nwalkers, init_sampler, stretch, Ncm.FitRunMsgs.SIMPLE)\n",
    "esmcmc.set_data_file (\"fig4_fit_esmcmc_out_aps.fits\")\n",
    "esmcmc.set_auto_trim (True)\n",
    "esmcmc.set_auto_trim_div (100)\n",
    "esmcmc.set_max_runs_time (2.0 * 60.0)\n",
    "\n",
    "esmcmc.start_run ()\n",
    "esmcmc.run_lre (20, 1.0e-3)\n",
    "esmcmc.end_run ()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the results:  below we plot the results of both MCMC\n",
    "\n",
    "The wrong analysis (blue) has a strong bias in $\\log_{10}(M_\\Delta)$. The peak with the wrong model is more than $3\\sigma$ away from the input values (green lines). The \"correct\" model used in the unbinned analysis (purple) yield results in agreement with the truth and has a much smaller variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2d1 = chi2.cdf (1.0, df = 2)\n",
    "s2d2 = chi2.cdf (4.0, df = 2)\n",
    "s2d3 = chi2.cdf (9.0, df = 2)\n",
    "\n",
    "\n",
    "rows = np.array ([mcat_wrong.peek_row (i).dup_array () for i in range (nwalkers * 10, mcat_wrong.len ())])\n",
    "params = [\"$\" + mcat_wrong.col_symb (i) + \"$\" for i in range (mcat_wrong.ncols ())]\n",
    "figure = corner.corner (rows[:,1:], labels = params[1:],# range=[(2.9, 9.5), (14.8, 15.12)],\n",
    "               color = (0.1, 0.2, 0.5, 0.5),levels = (s2d1, s2d2, s2d3),\n",
    "               bins = 40, smooth = 0.8, smooth1d = 0.8, range=[(2.9, 9.5), (14.8, 15.15)],\n",
    "               )\n",
    "\n",
    "mcat = esmcmc.peek_catalog ()\n",
    "rows = np.array ([mcat.peek_row (i).dup_array () for i in range (nwalkers * 10, mcat.len ())])\n",
    "params = [\"$\" + mcat.col_symb (i) + \"$\" for i in range (mcat.ncols ())]\n",
    "corner.corner (rows[:,1:], labels = params[1:],# reverse = False,\n",
    "                        truths=(4,15), truth_color='green', levels = (s2d1, s2d2, s2d3),\n",
    "                        bins = 40, smooth = 0.8, smooth1d = 0.8, \n",
    "                        color = (0.5, 0.2, 0.5, 1.0),\n",
    "                        label_kwargs=dict(fontsize=fsize), range=[(2.9, 9.5), (14.8, 15.15)],\n",
    "                        fig = figure)\n",
    "\n",
    "figure.set_size_inches(7, 7)\n",
    "figure.tight_layout()\n",
    "plt.savefig('Fig4.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2d1 = chi2.cdf (1.0, df = 2)\n",
    "s2d2 = chi2.cdf (4.0, df = 2)\n",
    "s2d3 = chi2.cdf (9.0, df = 2)\n",
    "\n",
    "\n",
    "rows = np.array ([mcat_wrong.peek_row (i).dup_array () for i in range (nwalkers * 10, mcat_wrong.len ())])\n",
    "params = [\"$\" + mcat_wrong.col_symb (i) + \"$\" for i in range (mcat_wrong.ncols ())]\n",
    "figure = corner.corner (rows[:,1:], labels = params[1:],# range=[(2.9, 9.5), (14.8, 15.12)],\n",
    "               color = (0.1, 0.2, 0.5, 0.5),levels = (s2d1, s2d2, s2d3),\n",
    "               bins = 40, smooth = 0.8, smooth1d = 0.8, range=[(2.9, 9.5), (14.8, 15.15)],\n",
    "                hist_kwargs={'lw':.8}\n",
    "               )\n",
    "\n",
    "mcat = esmcmc.peek_catalog ()\n",
    "rows = np.array ([mcat.peek_row (i).dup_array () for i in range (nwalkers * 10, mcat.len ())])\n",
    "params = [\"$\" + mcat.col_symb (i) + \"$\" for i in range (mcat.ncols ())]\n",
    "corner.corner (rows[:,1:], labels = params[1:],# reverse = False,\n",
    "                        truths=(4,15), truth_color='green', levels = (s2d1, s2d2, s2d3),\n",
    "                        bins = 40, smooth = 0.8, smooth1d = 0.8, \n",
    "                        color = (0.5, 0.2, 0.5, 1.0),\n",
    "                        label_kwargs=dict(fontsize=10), range=[(2.9, 9.5), (14.8, 15.15)],\n",
    "                        hist_kwargs={'lw':.8},\n",
    "                        fig = figure)\n",
    "\n",
    "figure.set_size_inches(8/2.54, 8/2.54)\n",
    "\n",
    "axes = figure.axes\n",
    "for ax in axes:\n",
    "    ax.xaxis.grid(True, which='major', lw=.5)\n",
    "    ax.yaxis.grid(True, which='major', lw=.5)\n",
    "    ax.xaxis.grid(True, which='minor', lw=.1)\n",
    "    ax.yaxis.grid(True, which='minor', lw=.1)\n",
    "for ax in (axes[0], axes[2]):\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(.5))\n",
    "axes[2].yaxis.set_minor_locator(MultipleLocator(.02))\n",
    "axes[3].xaxis.set_minor_locator(MultipleLocator(.02))\n",
    "\n",
    "axes[2].yaxis.set_label_coords(-.4,.5)\n",
    "axes[2].xaxis.set_label_coords(.5,-.4)\n",
    "axes[3].xaxis.set_label_coords(.5,-.4)\n",
    "\n",
    "for ax in axes:\n",
    "    for col in ax.collections:\n",
    "        col.set_linewidth(.8)\n",
    "    for line in ax.lines:\n",
    "        line.set_linewidth(.8)\n",
    "        line.set_markersize(2)\n",
    "\n",
    "figure.subplots_adjust(left=.2, bottom=.2, right=.98, top=.98)\n",
    "figure.savefig('Fig4.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = Ncm.Serialize.new (0)\n",
    "data = fit.lh.dset.get_data (0)\n",
    "ser.to_file (data, \"Fig4_data.obj\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
