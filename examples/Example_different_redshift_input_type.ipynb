{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model profiles using different type of source redshift information as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we model lensing profiles by giving as input either : \n",
    "- discrete source redshifts, \n",
    "- a redshift distribution function,\n",
    "- the value of the mean beta parameters : \n",
    "$\\langle \\beta_s \\rangle = \\left\\langle \\frac{D_{LS}}{D_S}\\frac{D_\\infty}{D_{L,\\infty}}\\right\\rangle$ ,\n",
    "$\\langle \\beta_s^2 \\rangle = \\left\\langle \\left(\\frac{D_{LS}}{D_S}\\frac{D_\\infty}{D_{L,\\infty}}\\right)^2 \\right\\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message='.*(!).*')\n",
    "import os\n",
    "## Uncomment the following line if you want to use a specific modeling backend among 'ct' (cluster-toolkit), 'ccl' (CCL) or 'nc' (Numcosmo). Default is 'ccl'\n",
    "#os.environ['CLMM_MODELING_BACKEND'] = 'nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: import clmm\n",
    "except:\n",
    "    import notebook_install\n",
    "    notebook_install.install_clmm_pipeline(upgrade=False)\n",
    "    import clmm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "matplotlib.rcParams.update({'figure.figsize': (20,6)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we know which version we're using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clmm.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import mock data module and setup the configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clmm.support import mock_data as mock\n",
    "from clmm import Cosmology\n",
    "from clmm.z_distributions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mock data generation requires a defined cosmology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_cosmo = Cosmology(H0 = 70.0, Omega_dm0 = 0.27 - 0.045, Omega_b0 = 0.045, Omega_k0 = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mock data generation requires some cluster information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmo = mock_cosmo\n",
    "\n",
    "# cluster properties from https://arxiv.org/pdf/1611.03866.pdf\n",
    "cluster_id = \"SPT-CL J0000âˆ’5748\"\n",
    "cluster_m = 4.56e14 # M500,c\n",
    "cluster_z = 0.702\n",
    "cluster_ra = 0.2499 \n",
    "cluster_dec = -57.8064\n",
    "concentration = 5 # (arbitrary value, not from the paper)\n",
    "\n",
    "#source redshift distribution properties\n",
    "cluster_beta_s_mean = 0.466\n",
    "cluster_beta_s2_mean = 0.243\n",
    "ngal_density= 26. *100 # density of source galaxies per arcmin^2 # (arbitrary value, not from the paper)\n",
    "model_z_distrib_dict = {'func':desc_srd, 'name':\"desc_srd\"}\n",
    "delta_z_cut = 0.1\n",
    "zsrc_min = cluster_z + delta_z_cut\n",
    "zsrc_max = 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Defining the different inputs for the source redshifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete redshifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the mock source catalog\n",
    "\n",
    "The CLMM mock data generation will provide, among other things, a redshift value for each background galaxy that is draw from the redshift distribution given by `model_z_distrib_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "source_catalog = mock.generate_galaxy_catalog(cluster_m, cluster_z, concentration, cosmo, model_z_distrib_dict['name'], delta_so=500,\n",
    "                                              massdef='critical', zsrc_min=zsrc_min, zsrc_max=zsrc_max, ngal_density=ngal_density, \n",
    "                                              cluster_ra=cluster_ra, cluster_dec=cluster_dec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta parameters\n",
    "From this udnerlying redshift distribution, one may directly compute the average $\\langle\\beta_s\\rangle$ and $\\langle\\beta_s^2\\rangle$ quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_inf = 1000\n",
    "\n",
    "beta_s_mean        = clmm.utils.compute_beta_s_mean(cluster_z, z_inf, cosmo, zmax=zsrc_max, delta_z_cut=delta_z_cut, zmin=None, z_distrib_func=model_z_distrib_dict['func'])\n",
    "beta_s_square_mean = clmm.utils.compute_beta_s_square_mean(cluster_z, z_inf, cosmo, zmax=zsrc_max, delta_z_cut=delta_z_cut, zmin=None, z_distrib_func=model_z_distrib_dict['func'])\n",
    "\n",
    "print (\"$<\\\\beta_s>$ = \", round(beta_s_mean,3), \",\\n$<\\\\beta_s^2>$ = \", round(beta_s_square_mean,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(0,zsrc_max,1000)\n",
    "\n",
    "plt.hist(source_catalog['z'], bins=50, alpha=0.3, density=True, label='mock source redshift');\n",
    "plt.axvline(zsrc_min, color='red', label='requested zmin')\n",
    "plt.text(4, 0.6, '$\\\\langle\\\\beta_s\\\\rangle$ = '+str(round(beta_s_mean,3)) + ', $\\\\langle\\\\beta_s^2\\\\rangle$ = ' + str(round(beta_s_square_mean,3)))\n",
    "#here we multiply by a constant for visualisation purposes\n",
    "plt.plot(z,model_z_distrib_dict['func'](z)*25, linestyle='dashed',label='redshift distribution fct (arbitrary normalisation)')\n",
    "plt.xlabel('$z_s$')\n",
    "plt.ylabel('pdf')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a zmax=3 (different from default values) to highlight the importance of specifying the zmax when computing the modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Compute models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are computing the models for the tangential shear, reduced shear, convergence, magnification and magnification bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile from mock data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compute the cluster profile based on the mock source catalog produced in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_object = clmm.GalaxyCluster(cluster_id, cluster_ra, cluster_dec, \n",
    "                               cluster_z, source_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_object.compute_tangential_and_cross_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_object.make_radial_profile('Mpc', bins=10, cosmo=cosmo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ways of modeling the reduced shear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define radius\n",
    "rr = np.logspace(np.log10(0.2), np.log10(5), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Case 1 : Discrete redshift and exact formula**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In case we know the discrete source redshift, we can compute the reduced shear for each source galaxy and take the average at a given radius. \n",
    "#### This may take a bit of time, depending on the size of source redshift catalog and number of radius points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#in case we know the discrete source redshift, we can compute the reduced shear for each source galaxy and take the average at a given radius.\n",
    "gt_discrete = np.zeros(rr.size)\n",
    "\n",
    "for i in range(rr.size):\n",
    "    gt_discrete[i] = np.mean(clmm.theory.compute_reduced_tangential_shear(rr[i], cluster_m, concentration, cluster_z, \n",
    "                                            source_catalog['z'], cosmo, delta_mdef=500,  massdef='critical', z_src_info='discrete', approx=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Case 2 : Redshift distribution and exact formula**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we don't know the exact source redshift but we know the source mean redshift distribution function, we can give it as input. \\\\\n",
    "#### In this case, we are integrating over the distribution function so it may be quite slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gt_distribution_no_approx = clmm.theory.compute_reduced_tangential_shear(rr, cluster_m, concentration, cluster_z, \n",
    "                                            model_z_distrib_dict['func'], cosmo, delta_mdef=500,  massdef='critical', z_src_info='distribution', beta_kwargs={'zmax':zsrc_max}, approx=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cases 3 : Redshift distribution and approximation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we want a faster approach we can use an approximation for the reduced shear, using 1 or 2 order of Taylor expansion for the expression of the reduced shear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gt_distribution_order1 = clmm.theory.compute_reduced_tangential_shear(rr, cluster_m, concentration, cluster_z, \n",
    "                                            model_z_distrib_dict['func'], cosmo, delta_mdef=500,  massdef='critical', z_src_info='distribution', beta_kwargs={'zmax':zsrc_max},  approx=\"order1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gt_distribution_order2 = clmm.theory.compute_reduced_tangential_shear(rr, cluster_m, concentration, cluster_z, \n",
    "                                            model_z_distrib_dict['func'], cosmo, delta_mdef=500,  massdef='critical', z_src_info='distribution', beta_kwargs={'zmax':zsrc_max},  approx=\"order2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cases 4 : Mean lensing efficiencies and approximation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also model the reduced shear if the only information we have about the source redshift distribution is through the mean lensing efficiency parameters $\\langle\\beta_s\\rangle$ and $\\langle\\beta_s^2\\rangle$. \\\n",
    "In this case, we need to use an approximation for the formula. This is the fastest approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gt_beta_1 = clmm.theory.compute_reduced_tangential_shear(rr, cluster_m, concentration, cluster_z, \n",
    "                                            [beta_s_mean, beta_s_square_mean], cosmo, delta_mdef=500,  massdef='critical', beta_kwargs={'zmax':zsrc_max}, z_src_info='beta', approx=\"order1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gt_beta_2 = clmm.theory.compute_reduced_tangential_shear(rr, cluster_m, concentration, cluster_z, \n",
    "                                            [beta_s_mean, beta_s_square_mean], cosmo, delta_mdef=500,  massdef='critical', beta_kwargs={'zmax':zsrc_max}, z_src_info='beta', approx=\"order2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Comparison of the four cases for the reduced tangnetial shear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,sharex=True)\n",
    "\n",
    "ax1.loglog(gc_object.profile['radius'], gc_object.profile['gt'], 'gs-', label = 'data')\n",
    "\n",
    "ax1.loglog(rr, gt_discrete, 'k.-', label = 'discrete')\n",
    "ax1.loglog(rr, gt_distribution_no_approx, 'r.-', label = 'distribution, no approx')\n",
    "ax1.loglog(rr, gt_distribution_order1, 'ro-', label = 'distribution, order 1 approx')\n",
    "ax1.loglog(rr, gt_distribution_order2, 'rd-', label = 'distribution, order 2 approx')\n",
    "ax1.loglog(rr, gt_beta_1, 'b--', label = 'beta, order 1 approx')\n",
    "ax1.loglog(rr, gt_beta_2, 'bx-', label = 'beta, order 2 approx')\n",
    "\n",
    "ax1.set_ylabel('$g_t$')\n",
    "ax1.set_xlabel('radius [Mpc]')\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(rr, 100*(gt_discrete-gt_discrete)/gt_discrete, 'k.-', label = 'discrete')\n",
    "ax2.plot(rr, 100*(gt_distribution_no_approx-gt_discrete)/gt_discrete, 'r-.', label = 'distribution, no approx')\n",
    "ax2.plot(rr, 100*(gt_distribution_order1-gt_discrete)/gt_discrete, 'ro-', label = 'distribution, order 1 approx')\n",
    "ax2.plot(rr, 100*(gt_distribution_order2-gt_discrete)/gt_discrete, 'rd-', label = 'distribution, order 2 approx')\n",
    "ax2.plot(rr, 100*(gt_beta_1-gt_discrete)/gt_discrete, 'b--', label = 'beta, order 1 approx')\n",
    "ax2.plot(rr, 100*(gt_beta_2-gt_discrete)/gt_discrete, 'bx-', label = 'beta, order 2 approx')\n",
    "\n",
    "ax2.set_ylabel('%')\n",
    "ax2.set_xlabel('radius [Mpc]')\n",
    "\n",
    "ax2.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All modeled profiles give similar results. They do not correspond to the profile computed from the data in the inner part, because of different ways of constructing the profiles (taking the average radial point and reduced shear value in a bin or computing the average expected reduced shear at a given radius).\n",
    "\n",
    "The profiles computed using an approximation for the reduced shear formula are lower by a few percents, especially in the inner region. The profiles computed from a redshift distribution or a known source redshift differ at the subpercent level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Modeling and plotting the other quantities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will just compute models for cases 1, 2 and 4. For the shear and convergence there is no need for an approximated formula. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammat_discrete = np.zeros(rr.size)\n",
    "\n",
    "for i in range(rr.size):\n",
    "    gammat_discrete[i] = np.mean(clmm.theory.compute_tangential_shear(rr[i], cluster_m, concentration, cluster_z, \n",
    "                                            source_catalog['z'], cosmo, delta_mdef=500,  massdef='critical', z_src_info='discrete'))\n",
    "    \n",
    "gammat_distribution = clmm.theory.compute_tangential_shear(rr, cluster_m, concentration, cluster_z, \n",
    "                                            model_z_distrib_dict['func'], cosmo, delta_mdef=500,  massdef='critical', z_src_info='distribution', beta_kwargs={'zmax':zsrc_max})\n",
    "    \n",
    "gammat_beta = clmm.theory.compute_tangential_shear(rr, cluster_m, concentration, cluster_z, \n",
    "                                            [beta_s_mean, beta_s_square_mean], cosmo, delta_mdef=500,  massdef='critical', z_src_info='beta', beta_kwargs={'zmax':zsrc_max})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,sharex=True)\n",
    "\n",
    "ax1.loglog(rr, gammat_discrete, 'k.-', label = 'discrete')\n",
    "ax1.loglog(rr, gammat_distribution, 'rx-', label = 'distribution, no approx')\n",
    "ax1.loglog(rr, gammat_beta, 'b--', label = 'beta, no approx')\n",
    "\n",
    "ax1.set_ylabel('$\\\\gamma_t$')\n",
    "ax1.set_xlabel('radius [Mpc]')\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(rr, 100*(gammat_discrete-gammat_discrete)/gammat_discrete, 'k.-', label = 'discrete')\n",
    "ax2.plot(rr, 100*(gammat_distribution-gammat_discrete)/gammat_discrete, 'rx-', label = 'distribution, no approx')\n",
    "ax2.plot(rr, 100*(gammat_beta-gammat_discrete)/gammat_discrete, 'b--', label = 'beta, no approx')\n",
    "\n",
    "ax2.set_ylabel('%')\n",
    "ax2.set_xlabel('radius [Mpc]')\n",
    "\n",
    "ax2.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_discrete = np.zeros(rr.size)\n",
    "\n",
    "for i in range(rr.size):\n",
    "    kappa_discrete[i] = np.mean(clmm.theory.compute_convergence(rr[i], cluster_m, concentration, cluster_z, \n",
    "                                            source_catalog['z'], cosmo, delta_mdef=500,  massdef='critical', z_src_info='discrete'))\n",
    "    \n",
    "kappa_distribution = clmm.theory.compute_convergence(rr, cluster_m, concentration, cluster_z, \n",
    "                                            model_z_distrib_dict['func'], cosmo, delta_mdef=500,  massdef='critical', z_src_info='distribution', beta_kwargs={'zmax':zsrc_max})\n",
    "    \n",
    "kappa_beta = clmm.theory.compute_convergence(rr, cluster_m, concentration, cluster_z, \n",
    "                                            [beta_s_mean, beta_s_square_mean], cosmo, delta_mdef=500,  massdef='critical', z_src_info='beta', beta_kwargs={'zmax':zsrc_max})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,sharex=True)\n",
    "\n",
    "ax1.loglog(rr, kappa_discrete, 'k.-', label = 'discrete')\n",
    "ax1.loglog(rr, kappa_distribution, 'rx-', label = 'distribution, no approx')\n",
    "ax1.loglog(rr, kappa_beta, 'b--', label = 'beta, no approx')\n",
    "\n",
    "ax1.set_ylabel('$\\\\kappa$')\n",
    "ax1.set_xlabel('radius [Mpc]')\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(rr, 100*(kappa_discrete-kappa_discrete)/kappa_discrete, 'k.-', label = 'discrete')\n",
    "ax2.plot(rr, 100*(kappa_distribution-kappa_discrete)/kappa_discrete, 'rx-', label = 'distribution, no approx')\n",
    "ax2.plot(rr, 100*(kappa_beta-kappa_discrete)/kappa_discrete, 'b--', label = 'beta, no approx')\n",
    "\n",
    "ax2.set_ylabel('%')\n",
    "ax2.set_xlabel('radius [Mpc]')\n",
    "\n",
    "ax2.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_discrete = np.zeros(rr.size)\n",
    "\n",
    "for i in range(rr.size):\n",
    "    mu_discrete[i] = np.mean(clmm.theory.compute_magnification(rr[i], cluster_m, concentration, cluster_z, \n",
    "                                            source_catalog['z'], cosmo, delta_mdef=500,  massdef='critical', z_src_info='discrete'))\n",
    "    \n",
    "mu_distribution = clmm.theory.compute_magnification(rr, cluster_m, concentration, cluster_z, \n",
    "                                            model_z_distrib_dict['func'], cosmo, delta_mdef=500,  massdef='critical', z_src_info='distribution', approx=None, beta_kwargs={'zmax':zsrc_max})\n",
    "    \n",
    "mu_beta_1 = clmm.theory.compute_magnification(rr, cluster_m, concentration, cluster_z, \n",
    "                                            [beta_s_mean, beta_s_square_mean], cosmo, delta_mdef=500,  massdef='critical', z_src_info='beta',  approx=\"order1\", beta_kwargs={'zmax':zsrc_max})\n",
    "\n",
    "mu_beta_2 = clmm.theory.compute_magnification(rr, cluster_m, concentration, cluster_z, \n",
    "                                            [beta_s_mean, beta_s_square_mean], cosmo, delta_mdef=500,  massdef='critical', z_src_info='beta',  approx=\"order2\", beta_kwargs={'zmax':zsrc_max})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,sharex=True)\n",
    "\n",
    "ax1.loglog(rr, mu_discrete, 'k.-', label = 'discrete')\n",
    "ax1.loglog(rr, mu_distribution, 'rs--', label = 'distribution, no approx')\n",
    "ax1.loglog(rr, mu_beta_1, 'bx-', label = 'beta, order 1 approx')\n",
    "ax1.loglog(rr, mu_beta_2, 'b--', label = 'beta, order 2 approx')\n",
    "\n",
    "ax1.set_ylabel('$\\\\mu$')\n",
    "ax1.set_xlabel('radius [Mpc]')\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(rr, 100*(mu_discrete-mu_discrete)/mu_discrete, 'k.-', label = 'discrete')\n",
    "ax2.plot(rr, 100*(mu_distribution-mu_discrete)/mu_discrete, 'rs--', label = 'distribution, no approx')\n",
    "ax2.plot(rr, 100*(mu_beta_1-mu_discrete)/mu_discrete, 'bx-', label = 'beta, order 1 approx')\n",
    "ax2.plot(rr, 100*(mu_beta_2-mu_discrete)/mu_discrete, 'b--', label = 'beta, order 2 approx')\n",
    "\n",
    "ax2.set_ylabel('%')\n",
    "ax2.set_xlabel('radius [Mpc]')\n",
    "\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_bias_discrete = np.zeros(rr.size)\n",
    "\n",
    "alpha = 2.7 #arbitrary value for the slope of the number density\n",
    "\n",
    "for i in range(rr.size):\n",
    "    mu_bias_discrete[i] = np.mean(clmm.theory.compute_magnification_bias(rr[i], alpha, cluster_m, concentration, cluster_z, \n",
    "                                            source_catalog['z'], cosmo, delta_mdef=500,  massdef='critical', z_src_info='discrete'))\n",
    "    \n",
    "mu_bias_distribution = clmm.theory.compute_magnification_bias(rr, alpha, cluster_m, concentration, cluster_z, \n",
    "                                            model_z_distrib_dict['func'], cosmo, delta_mdef=500,  massdef='critical', z_src_info='distribution', approx=None, beta_kwargs={'zmax':zsrc_max})\n",
    "    \n",
    "mu_bias_beta_1 = clmm.theory.compute_magnification_bias(rr, alpha, cluster_m, concentration, cluster_z, \n",
    "                                            [beta_s_mean, beta_s_square_mean], cosmo, delta_mdef=500,  massdef='critical', z_src_info='beta',  approx=\"order1\", beta_kwargs={'zmax':zsrc_max})\n",
    "\n",
    "mu_bias_beta_2 = clmm.theory.compute_magnification_bias(rr, alpha, cluster_m, concentration, cluster_z, \n",
    "                                            [beta_s_mean, beta_s_square_mean], cosmo, delta_mdef=500,  massdef='critical', z_src_info='beta',  approx=\"order2\", beta_kwargs={'zmax':zsrc_max})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,sharex=True)\n",
    "\n",
    "ax1.loglog(rr, mu_bias_discrete, 'k.-', label = 'discrete')\n",
    "ax1.loglog(rr, mu_bias_distribution, 'rs--', label = 'distribution, no approx')\n",
    "ax1.loglog(rr, mu_bias_beta_1, 'bx-', label = 'beta, order 1 approx')\n",
    "ax1.loglog(rr, mu_bias_beta_2, 'b--', label = 'beta, order 2 approx')\n",
    "\n",
    "\n",
    "ax1.set_ylabel('$\\\\delta_{\\\\mu} + 1$')\n",
    "ax1.set_xlabel('radius [Mpc]')\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(rr, 100*(mu_bias_discrete-mu_bias_discrete)/mu_bias_discrete, 'k.-', label = 'discrete')\n",
    "ax2.plot(rr, 100*(mu_bias_distribution-mu_bias_discrete)/mu_bias_discrete, 'rs--', label = 'distribution, no approx')\n",
    "ax2.plot(rr, 100*(mu_bias_beta_1-mu_bias_discrete)/mu_bias_discrete, 'bx-', label = 'beta, order 1 approx')\n",
    "ax2.plot(rr, 100*(mu_bias_beta_2-mu_bias_discrete)/mu_bias_discrete, 'b--', label = 'beta, order 2 approx')\n",
    "\n",
    "ax2.set_ylabel('%')\n",
    "ax2.set_xlabel('radius [Mpc]')\n",
    "\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When making approximation, there may be large differences (~few 10% level) in the inner regions (compare to when using the exact redshift of the sources), especially for magnification and magnification bias. However, these approaches are very fast to compute. The user as to chose the appropriate method depending on the use case.**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "wrk",
   "language": "python",
   "name": "wrk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
